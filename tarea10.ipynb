{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función, gradiente y Hessiano\n",
    "# def f(x):\n",
    "#     \"\"\"Funcion a Evaluar\"\"\"\n",
    "#     f = 2*x[0]**2+2*x[0]*x[1]+10*x[1]**2 + 20 + 3*x[0]-4*x[1]\n",
    "#     return f\n",
    "\n",
    "\n",
    "# gradiente  ∇f\n",
    "def grad(x):\n",
    "    g = np.array([\n",
    "        3*x[0] - x[1] + -2*x[2] - 1,\n",
    "        -x[0] + 4*x[1] + -3*x[2],\n",
    "        -2*x[0] - 3*x[1] + 6*x[2] - 3,\n",
    "    ])\n",
    "    return g\n",
    "\n",
    "\n",
    "def hessiano(x):\n",
    "    # return axay\n",
    "    return np.array([\n",
    "        [3, -1, -2],\n",
    "        [-1, 4, -3],\n",
    "        [-2, -3, 6]\n",
    "    ])\n",
    "\n",
    "\n",
    "# dirección del gradiente p\n",
    "def dirgrad(x):\n",
    "    vgrad = grad(x)\n",
    "    magGrad = np.sqrt(vgrad.dot(vgrad))\n",
    "    p = -vgrad/magGrad\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradiente Conjugado Preliminar\n",
    "# def almd(x0, r, p):\n",
    "#     return -np.dot(r, p) / np.dot(np.dot(hessiano(x0), p), p)\n",
    "\n",
    "\n",
    "# def beta(x0, r, p):\n",
    "#     return np.dot(np.dot(hessiano(x0), p), r)/np.dot(np.dot(hessiano(x0), p), p)\n",
    "\n",
    "\n",
    "# def gradConjugadoPreliminar(x0, b, k=0, tol=1e-3):\n",
    "#     r = grad(x0)\n",
    "#     p = r*-1\n",
    "#     print(\"k,x0, f(x^k), aMD, b\")\n",
    "#     while np.linalg.norm(grad(x0)) >= tol:\n",
    "#         aMD = almd(x0, r, p)\n",
    "#         x0 = x0 + aMD*p\n",
    "#         r = np.dot(hessiano(x0), x0) - b\n",
    "#         b = beta(x0, r, p)\n",
    "#         p = -r + b*p\n",
    "#         # print(x0, f(x0), aMD, b)\n",
    "#         print(k, x0,  aMD, b)\n",
    "#         k+=1\n",
    "#     return x0\n",
    "\n",
    "\n",
    "def gradConjugadoPreliminar(x0, k=0, tol=1e-3):\n",
    "    r = grad(x0)\n",
    "    p = r*-1\n",
    "    while np.linalg.norm(grad(x0)) >= tol:\n",
    "        alpha = -np.dot(r, p) / np.dot(p, np.dot(hessiano(x0), p),)\n",
    "        x0 = x0 + alpha*p\n",
    "        r = grad(x0)\n",
    "        b = np.dot(r, np.dot(hessiano(x0), p),) / \\\n",
    "            np.dot(p, np.dot(hessiano(x0), p),)\n",
    "        p = (r*-1)+b*p\n",
    "        print(k, x0,  alpha, b, np.dot(hessiano(x0), p))\n",
    "       \n",
    "        k +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<==Preliminar==>\n",
      "0 [10.15384615 10.          8.92307692] 0.15384615384615385 0.24260355029585748 [ 2.81656805 -5.14792899  0.40236686]\n",
      "1 [8.66666667 6.66666667 6.83333333] 1.0833333333333321 2.347222222222219 [-8.55555556  4.58333333 -1.22222222]\n",
      "2 [4.36363636 4.09090909 4.        ] 0.5454545454545464 3.743347095711843e-16 [-3.51770645e-14 -6.57858591e-14  1.13229318e-13]\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([10,10,10])\n",
    "b = [1, 0, 3]\n",
    "\n",
    "print(\"<==Preliminar==>\")\n",
    "gradConjugadoPreliminar(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradiente Conjugado Final\n",
    "\n",
    "def gradienteConjugado(x0, b, k=0, tol=1e-6):\n",
    "    r = np.dot(hessiano(x0),x0)-b\n",
    "    p = r*-1\n",
    "    print(x0)\n",
    "    print(\"k, x0, alpha, beta, Ap\")\n",
    "    rDotr = np.dot(r, r)\n",
    "    AdotP = np.dot(hessiano(x0), p)\n",
    "    while np.linalg.norm(r) >= tol:\n",
    "        alpha = rDotr / np.dot(AdotP, p)\n",
    "        x0 = x0 + alpha*p\n",
    "        r1 = r + alpha * AdotP\n",
    "        b = (np.dot(r1, r1))/rDotr\n",
    "        p = -r1 + b*p\n",
    "        # print(x0, f(x0), alpha, b)\n",
    "        print(k,x0, alpha, b,AdotP)\n",
    "        r = r1\n",
    "        rDotr = np.dot(r, r)\n",
    "        AdotP = np.dot(hessiano(x0), p)\n",
    "        k += 1\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<==Determinante ==>\n",
      "11.000000000000007\n",
      "<==Grad Conjugado==>\n",
      "[10 10 10]\n",
      "k, x0, alpha, beta, Ap\n",
      "0 [10.15384615 10.          8.92307692] 0.15384615384615385 0.242603550295858 [ 17  20 -44]\n",
      "1 [8.66666667 6.66666667 6.83333333] 1.0833333333333333 2.3472222222222223 [ 2.81656805 -5.14792899  0.40236686]\n",
      "2 [4.36363636 4.09090909 4.        ] 0.5454545454545455 1.116911598733652e-30 [-8.55555556  4.58333333 -1.22222222]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4.36363636, 4.09090909, 4.        ])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([10,10,10])\n",
    "print(\"<==Determinante ==>\")\n",
    "print(np.linalg.det(hessiano(x0)))\n",
    "\n",
    "b = [1, 0, 3]\n",
    "print(\"<==Grad Conjugado==>\")\n",
    "gradienteConjugado(x0, b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbd870ce3ecbb50051e36c03c034f91bd0ffd37935fd5f5a0057175bdb6e15f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
