{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 4*x[0]**2 + 3*x[0]*x[1] + 3*x[1]**2 + 2*x[0] + 3*x[1] + 3\n",
    "\n",
    "\n",
    "def grad(x):\n",
    "    return np.array([\n",
    "        8*x[0]+3*x[1]+2,\n",
    "        6*x[1]+3*x[0]+3,\n",
    "    ])\n",
    "\n",
    "\n",
    "def hessiano(x):\n",
    "    return np.array([\n",
    "        [8, 3],\n",
    "        [3, 6]\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradienteConjugado(x0, b, k=0, tol=1e-6):\n",
    "    r = grad(x0)\n",
    "    p = r*-1\n",
    "    print(x0)\n",
    "    print(\"x0, f(x^k), aMD, b\")\n",
    "    rDotr = np.dot(r, r)\n",
    "    AdotP = np.dot(hessiano(x0), p)\n",
    "    while np.linalg.norm(grad(x0)) >= tol:\n",
    "        alpha = rDotr / np.dot(AdotP, p)\n",
    "        x0 = x0 + alpha*p\n",
    "        r1 = r + alpha * AdotP\n",
    "        b = (np.dot(r1, r1))/rDotr\n",
    "        p = -r1 + b*p\n",
    "        print(x0, f(x0), alpha, b)\n",
    "        r = r1\n",
    "        rDotr = np.dot(r, r)\n",
    "        AdotP = np.dot(hessiano(x0), p)\n",
    "    return x0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtonNDim(x0, tol=1e-4):\n",
    "    k = 0\n",
    "    while np.linalg.norm(grad(x0)) > tol:\n",
    "        x0 = x0 - np.dot(np.linalg.inv(hessiano(x0)), grad(x0))\n",
    "        k = k+1\n",
    "        print(k, f(x0), x0)\n",
    "    return x0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<== GC ==>\n",
      "[0 0]\n",
      "x0, f(x^k), aMD, b\n",
      "[-0.21311475 -0.31967213] 2.307377049180328 0.10655737704918032 0.048978769148078465\n",
      "[-0.07692308 -0.46153846] 2.230769230769231 0.2406311637080868 3.871678047282085e-31\n",
      "<== Newton ==>\n",
      "1 2.230769230769231 [-0.07692308 -0.46153846]\n",
      "[-0.07692308 -0.46153846]\n",
      "<== gradiente fin ==>\n",
      "[0. 0.]\n",
      "<==Hessiano fin ==>\n",
      "38.99999999999999\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([0, 0])\n",
    "b = np.array([2, 3])\n",
    "print(\"<== GC ==>\")\n",
    "gc = gradienteConjugado(x0, b)\n",
    "# print(gc)\n",
    "print(\"<== Newton ==>\")\n",
    "print(newtonNDim(x0))\n",
    "\n",
    "print(\"<== gradiente fin ==>\")\n",
    "print(grad(gc))\n",
    "\n",
    "print(\"<==Hessiano fin ==>\")\n",
    "print(np.linalg.det(hessiano(gc)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [-0.21311475 -0.31967213] 2.3073770505859468\n",
      "2 [-0.07692308 -0.46153846] 2.230769230769231\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 2.230769\n",
      "         Iterations: 2\n",
      "         Function evaluations: 15\n",
      "         Gradient evaluations: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "     fun: 2.230769230769231\n",
       "     jac: array([5.96046448e-08, 5.96046448e-08])\n",
       " message: 'Optimization terminated successfully.'\n",
       "    nfev: 15\n",
       "     nit: 2\n",
       "    njev: 5\n",
       "  status: 0\n",
       " success: True\n",
       "       x: array([-0.07692308, -0.46153846])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nfeval = 1\n",
    "\n",
    "\n",
    "def callbackF(Xi):\n",
    "    global Nfeval\n",
    "    print(Nfeval, Xi, f(Xi))\n",
    "    Nfeval += 1\n",
    "\n",
    "\n",
    "optimize.minimize(f, x0, method='CG', callback=callbackF,\n",
    "                  options={\"disp\": True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    return np.array([\n",
    "        x[0]**2 + x[1]**2 - 2,\n",
    "        np.e**(x[0]-1) + x[1]**3 - 2\n",
    "    ])\n",
    "def hessiano(x):\n",
    "    return np.array(\n",
    "        [ \n",
    "            [2*x[0], 2*x[1]],\n",
    "            [np.e**(x[0]-1), 3*x[1]**2]\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculos de phi\n",
    "def phiAlpha(x0, alpha, p):\n",
    "    paX = x0 + p * alpha\n",
    "    return f(paX)\n",
    "\n",
    "\n",
    "def phipAlpha(x0, alpha, p):\n",
    "    x = x0 + alpha * p\n",
    "    vgrad = grad(x)\n",
    "    return (np.dot(vgrad, p))\n",
    "\n",
    "\n",
    "def phipp(x0, alpha, p):\n",
    "    x = x0 + alpha * p\n",
    "    ahess = hessiano(x)\n",
    "    return np.dot(np.dot(ahess, p), p)\n",
    "\n",
    "\n",
    "\n",
    "# Gradiente Conjugado NL\n",
    "def newton(xo, p, ao, itmax=100, tol=1e-12):\n",
    "    k = 0\n",
    "    # ak = 0\n",
    "    while abs(phipAlpha(xo, ao, p)) > tol:\n",
    "        phiap = phipAlpha(xo, ao, p)\n",
    "        phiapp = phipp(xo, ao, p)\n",
    "        ak = ao - phiap/phiapp\n",
    "        # print(f\"\\t {k}, {ak}\")\n",
    "        k = k+1\n",
    "        ao = ak\n",
    "        if k >= itmax:\n",
    "            print(\"Iteraciones exedidas\")\n",
    "            break\n",
    "    return ak\n",
    "\n",
    "\n",
    "def gradientesConjugados(x0, flavor=\"FR\", k=0, tol=1e-6):\n",
    "    beta = 0\n",
    "    p = -grad(x0)\n",
    "    print(\"k, fx, x0, alpha, beta, \")\n",
    "    while np.linalg.norm(grad(x0)) >= tol:\n",
    "        alpha = newton(x0, p, 0)\n",
    "        xi = x0 + alpha*p\n",
    "        gradxi = grad(xi)\n",
    "        gradx0 = grad(x0)\n",
    "        if flavor == \"FR\":\n",
    "            beta = np.dot(gradxi, gradxi)/np.dot(gradx0, gradx0)\n",
    "        elif flavor == \"PR\":\n",
    "            beta = np.dot(gradxi, (gradxi - gradx0)) / \\\n",
    "                (np.linalg.norm(gradx0))**2\n",
    "        elif flavor == \"PR+\":\n",
    "            beta = np.dot(gradxi, (gradxi - gradx0)) / \\\n",
    "                (np.linalg.norm(gradx0))**2\n",
    "            if beta < 0:\n",
    "                beta = 0\n",
    "        elif flavor == \"HS\":\n",
    "            beta = np.dot(gradxi, (gradxi - gradx0)) / \\\n",
    "                np.dot((gradxi - gradx0), p)\n",
    "        p = -gradxi + beta*p\n",
    "        print(k,\n",
    "        #  f(x0),\n",
    "          xi, alpha, beta)\n",
    "        x0 = xi\n",
    "        k += 1\n",
    "    return x0\n",
    "\n",
    "\n",
    "def bfgs(x0, tol=1e-6):\n",
    "    k = 0\n",
    "    a = np.identity(x0.size)\n",
    "    g0 = grad(x0)\n",
    "    # a = jHessFull(x0)\n",
    "    while (np.linalg.norm(grad(x0)) >= tol):\n",
    "        s = -np.dot(-grad(x0), a)\n",
    "        x1 = x0 - s\n",
    "        g1 = grad(x1)\n",
    "        y = g1 - g0\n",
    "        aux = s.T.dot(a).dot(s)\n",
    "        a = a + (np.dot(y, y)/np.dot(y, s)) - \\\n",
    "            np.dot(np.dot(a, s), np.dot(a, s))/np.dot(np.dot(a, s), s)\n",
    "        x0 = x1\n",
    "        print(k, x0, f(x0))\n",
    "        k = k+1\n",
    "        # print(a)\n",
    "    return x0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [ -9.         -24.71828183] 2735.2191335364028\n",
      "1 [-8043437.13733883 -8027658.15375513] 645827261219418.0\n",
      "2 [-3.33384719e+34 -3.33384719e+34] 1.1114537077841058e+70\n",
      "3 [-4.11839926e+172 -4.11839926e+172] inf\n",
      "4 [nan nan] nan\n",
      "k, fx, x0, alpha, beta, \n",
      "0 [1.16878298 0.90546295] 0.07556518330183765 4.498883645822914e-05\n",
      "1 [1.01861407 0.96389471] 0.8055766581685698 0.20810485070608264\n",
      "2 [1.01694439 0.99470223] 0.30575617486552314 0.17226544108783431\n",
      "3 [0.99987075 1.00588408] 0.6954884681462956 0.2663491661019688\n",
      "4 [0.99568251 1.00279326] 0.23161278390771853 0\n",
      "5 [0.99816642 0.99942742] 0.8219354351973654 1.3788992280672818\n",
      "6 [1.0008882  0.99879124] 0.30323860078273585 0\n",
      "7 [1.00106525 0.99954874] 0.27713782976479295 0.20228402838725612\n",
      "8 [1.00007246 1.00030703] 0.9024585359125166 0.5749135201196994\n",
      "9 [0.99975186 1.00018935] 0.2303925144914516 0\n",
      "10 [0.99981332 1.0000219 ] 0.5231847234220556 1.059960043444634\n",
      "11 [1.00003024 0.99991761] 0.4777617357290892 0\n",
      "12 [1.00005642 0.99997206] 0.2510336451707019 0.06898120048122083\n",
      "13 [1.00000685 1.00001424] 0.9959231517418473 0.7988410626167721\n",
      "14 [0.99998761 1.00001055] 0.23482533628368513 0\n",
      "15 [0.99998914 1.00000256] 0.4151575070941098 0.7446125160348268\n",
      "16 [1.0000008  0.99999585] 0.6021821179902704 0.11298304073868039\n",
      "17 [1.00000291 0.99999832] 0.2379687580709956 0.013976089410147998\n",
      "18 [1.00000053 1.00000062] 1.0148349312614735 0.9738609859844714\n",
      "19 [0.9999994  1.00000057] 0.24549895850705078 0\n",
      "20 [0.99999942 1.00000018] 0.3472907539963302 0.49944626794183833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_13052\\3343837133.py:3: RuntimeWarning: overflow encountered in double_scalars\n",
      "  x[0]**2 + x[1]**2 - 2,\n",
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_13052\\3343837133.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  np.e**(x[0]-1) + x[1]**3 - 2\n",
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_13052\\3996395181.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.dot(np.dot(a, s), np.dot(a, s))/np.dot(np.dot(a, s), s)\n",
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_13052\\4236520060.py:2: RuntimeWarning: overflow encountered in double_scalars\n",
      "  return 4*x[0]**2 + 3*x[0]*x[1] + 3*x[1]**2 + 2*x[0] + 3*x[1] + 3\n"
     ]
    }
   ],
   "source": [
    "x0 = np.array([2,3])\n",
    "\n",
    "bf = bfgs(x0)\n",
    "pr = gradientesConjugados(x0,\"PR+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.344172934156684e-07\n",
      "7.959922133913181e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.959922133913181e-07"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.linalg.norm(grad(bf))\n",
    "b = np.linalg.norm(grad(pr))\n",
    "print(a)\n",
    "print(b)\n",
    "np.amin([a,b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [2 2]\n",
      "1 [-15.   6.]\n",
      "2 [1.81562355 6.8935277 ]\n",
      "3 [ 2.1401269 -1.7359996]\n",
      "4 [1.92746611 3.80225459]\n",
      "5 [1.92884475 3.73705796]\n",
      "6 [1.92799524 3.72623921]\n",
      "7 [1.91214596 3.60462262]\n",
      "8 [1.87730953 3.40868998]\n",
      "9 [1.78078196 2.95674083]\n",
      "10 [1.64321543 2.41843247]\n",
      "11 [1.46734561 1.8778594 ]\n",
      "12 [1.28985069 1.53747275]\n",
      "13 [1.15504206 1.29320019]\n",
      "14 [1.03967453 1.04421659]\n",
      "15 [1.01397751 1.04530299]\n",
      "16 [0.9976235  0.99405575]\n",
      "17 [1.00005166 1.00016609]\n",
      "18 [0.99999891 0.9999994 ]\n",
      "[0.99999891 0.9999994 ]\n"
     ]
    }
   ],
   "source": [
    "# def grad(x):\n",
    "#     return np.array([\n",
    "#         x[0]**2 + x[1]**2 - 2,\n",
    "#         np.e**(x[0]-1) + x[1]**3 - 2\n",
    "#     ])\n",
    "\n",
    "\n",
    "# def hessiano(x):\n",
    "#     return np.array(\n",
    "#         [\n",
    "#             [2*x[0], 2*x[1]],\n",
    "#             [np.e**(x[0]-1), 3*x[1]**2]\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "def f(x):   # The rosenbrock function\n",
    "    return .5*(1 - x[0])**2 + (x[1] - x[0]**2)**2\n",
    "\n",
    "\n",
    "def fprime(x):\n",
    "    return np.array((-2*.5*(1 - x[0]) - 4*x[0]*(x[1] - x[0]**2), 2*(x[1] - x[0]**2)))\n",
    "\n",
    "\n",
    "\n",
    "def bfgs(x0, grad, hessian=None, maxiter=100, epsilon=1e-5):\n",
    "    x = x0\n",
    "    n = len(x0)\n",
    "    \n",
    "    if hessian is None:\n",
    "        hessian = np.eye(n)\n",
    "    for i in range(maxiter):\n",
    "        print(i, x)\n",
    "        gradient = grad(x)\n",
    "        if np.linalg.norm(gradient) < epsilon:\n",
    "            break\n",
    "        p = -np.dot(hessian, gradient)\n",
    "        x_new = x + p\n",
    "        s = x_new - x\n",
    "        y = grad(x_new) - gradient\n",
    "        rho = 1.0 / np.dot(y, s)\n",
    "        hessian = (np.eye(n) - rho * np.outer(s, y)) @ hessian @ (np.eye(n) -\n",
    "                                                                  rho * np.outer(y, s)) + rho * np.outer(s, s)\n",
    "        x = x_new\n",
    "    return x\n",
    "\n",
    "\n",
    "initial_guess = np.array([2, 2])\n",
    "\n",
    "result = bfgs(initial_guess, fprime)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "62ed03cf0ae78fa95a71efbccdd357a71f03a6d7f643ed6449667ba12cd077d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
